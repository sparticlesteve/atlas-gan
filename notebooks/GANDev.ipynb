{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development notebook for GAN architecture\n",
    "\n",
    "I'll use this notebook to test out modifications to the DCGAN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "# Externals\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out different conv params\n",
    "\n",
    "Can I tweak the kernel sizes, strides, number of layers, and maintain a consistent input and output size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample image and noise for input\n",
    "img_size = 64\n",
    "x = Variable(torch.randn(n_samples, 1, img_size, img_size))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_dim = 64\n",
    "noise = Variable(torch.randn(n_samples, noise_dim, 1, 1))\n",
    "noise.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default layer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 1, 64, 64])\n",
      "conv1: torch.Size([1, 16, 32, 32])\n",
      "conv2: torch.Size([1, 32, 16, 16])\n",
      "conv3: torch.Size([1, 64, 8, 8])\n",
      "conv4: torch.Size([1, 128, 4, 4])\n",
      "conv5: torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Discriminator layer sizes (dropping BN and activations)\n",
    "ndf = 16\n",
    "print('input:', x.size())\n",
    "h = nn.Conv2d(1, ndf, 4, 2, 1)(x)\n",
    "print('conv1:', h.size())\n",
    "h = nn.Conv2d(ndf, ndf*2, 4, 2, 1)(h)\n",
    "print('conv2:', h.size())\n",
    "h = nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)(h)\n",
    "print('conv3:', h.size())\n",
    "h = nn.Conv2d(ndf*4, ndf*8, 4, 2, 1)(h)\n",
    "print('conv4:', h.size())\n",
    "d = nn.Conv2d(ndf*8, 1, 4, 1, 0)(h)\n",
    "print('conv5:', d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   torch.Size([1, 64, 1, 1])\n",
      "deconv1: torch.Size([1, 128, 4, 4])\n",
      "deconv2: torch.Size([1, 64, 8, 8])\n",
      "deconv3: torch.Size([1, 32, 16, 16])\n",
      "deconv4: torch.Size([1, 16, 32, 32])\n",
      "deconv5: torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Generator layer sizes\n",
    "ngf = 16\n",
    "print('input:  ', noise.size())\n",
    "h = nn.ConvTranspose2d(noise_dim, ngf*8, 4, 1, 0)(noise)\n",
    "print('deconv1:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1)(h)\n",
    "print('deconv2:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1)(h)\n",
    "print('deconv3:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1)(h)\n",
    "print('deconv4:', h.size())\n",
    "g = nn.ConvTranspose2d(ngf, 1, 4, 2, 1)(h)\n",
    "print('deconv5:', g.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With one extra layer in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 1, 64, 64])\n",
      "conv1: torch.Size([1, 16, 32, 32])\n",
      "conv2: torch.Size([1, 32, 16, 16])\n",
      "conv3: torch.Size([1, 64, 8, 8])\n",
      "conv4: torch.Size([1, 128, 4, 4])\n",
      "conv5: torch.Size([1, 256, 2, 2])\n",
      "conv6: torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Discriminator layer sizes (dropping BN and activations)\n",
    "ndf = 16\n",
    "print('input:', x.size())\n",
    "h = nn.Conv2d(1, ndf, 4, 2, 1)(x)\n",
    "print('conv1:', h.size())\n",
    "h = nn.Conv2d(ndf, ndf*2, 4, 2, 1)(h)\n",
    "print('conv2:', h.size())\n",
    "h = nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)(h)\n",
    "print('conv3:', h.size())\n",
    "h = nn.Conv2d(ndf*4, ndf*8, 4, 2, 1)(h)\n",
    "print('conv4:', h.size())\n",
    "h = nn.Conv2d(ndf*8, ndf*16, 4, 2, 1)(h)\n",
    "print('conv5:', h.size())\n",
    "d = nn.Conv2d(ndf*16, 1, 2, 1, 0)(h)\n",
    "print('conv6:', d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   torch.Size([1, 64, 1, 1])\n",
      "deconv1: torch.Size([1, 256, 2, 2])\n",
      "deconv2: torch.Size([1, 128, 4, 4])\n",
      "deconv3: torch.Size([1, 64, 8, 8])\n",
      "deconv4: torch.Size([1, 32, 16, 16])\n",
      "deconv5: torch.Size([1, 16, 32, 32])\n",
      "deconv6: torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Generator layer sizes\n",
    "ngf = 16\n",
    "print('input:  ', noise.size())\n",
    "h = nn.ConvTranspose2d(noise_dim, ngf*16, 2, 1, 0)(noise)\n",
    "print('deconv1:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*16, ngf*8, 4, 2, 1)(h)\n",
    "print('deconv2:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1)(h)\n",
    "print('deconv3:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1)(h)\n",
    "print('deconv4:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1)(h)\n",
    "print('deconv5:', h.size())\n",
    "g = nn.ConvTranspose2d(ngf, 1, 4, 2, 1)(h)\n",
    "print('deconv6:', g.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional GAN architecture\n",
    "\n",
    "In this case I will have 1-2 parameters on which to condition the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_dim = 2\n",
    "cond = Variable(torch.randn(n_samples, cond_dim))\n",
    "cond.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 1, 64, 64])\n",
      "conv1: torch.Size([1, 16, 32, 32])\n",
      "conv2: torch.Size([1, 32, 16, 16])\n",
      "conv3: torch.Size([1, 64, 8, 8])\n",
      "conv4: torch.Size([1, 128, 4, 4])\n",
      "flat:  torch.Size([1, 2048])\n",
      "concat: torch.Size([1, 2050])\n",
      "linear: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Discriminator layer sizes (dropping BN and activations)\n",
    "ndf = 16\n",
    "print('input:', x.size())\n",
    "h = nn.Conv2d(1, ndf, 4, 2, 1)(x)\n",
    "print('conv1:', h.size())\n",
    "h = nn.Conv2d(ndf, ndf*2, 4, 2, 1)(h)\n",
    "print('conv2:', h.size())\n",
    "h = nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)(h)\n",
    "print('conv3:', h.size())\n",
    "h = nn.Conv2d(ndf*4, ndf*8, 4, 2, 1)(h)\n",
    "print('conv4:', h.size())\n",
    "\n",
    "# This next bit is effectively just a FC layer.\n",
    "# Let's drop the pretense to make it easier to work with.\n",
    "#d = nn.Conv2d(ndf*8, 1, 4, 1, 0)(h)\n",
    "h = h.view(n_samples, -1)\n",
    "print('flat: ', h.size())\n",
    "h = torch.cat([h, cond], dim=1)\n",
    "print('concat:', h.size())\n",
    "\n",
    "d = nn.Linear(h.size(1), 1)(h)\n",
    "print('linear:', d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:   torch.Size([1, 66, 1, 1])\n",
      "deconv1: torch.Size([1, 128, 4, 4])\n",
      "deconv2: torch.Size([1, 64, 8, 8])\n",
      "deconv3: torch.Size([1, 32, 16, 16])\n",
      "deconv4: torch.Size([1, 16, 32, 32])\n",
      "deconv5: torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the noise and condition variables\n",
    "noise_cond = torch.cat([noise, cond[:, :, None, None]], dim=1)\n",
    "\n",
    "# Generator layer sizes\n",
    "ngf = 16\n",
    "print('input:  ', noise_cond.size())\n",
    "h = nn.ConvTranspose2d(noise_dim + cond_dim, ngf*8, 4, 1, 0)(noise_cond)\n",
    "print('deconv1:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1)(h)\n",
    "print('deconv2:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1)(h)\n",
    "print('deconv3:', h.size())\n",
    "h = nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1)(h)\n",
    "print('deconv4:', h.size())\n",
    "g = nn.ConvTranspose2d(ngf, 1, 4, 2, 1)(h)\n",
    "print('deconv5:', g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
