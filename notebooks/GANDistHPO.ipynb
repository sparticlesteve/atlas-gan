{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed hyper-parameter optimization example for the RPV GAN\n",
    "\n",
    "This notebook shows how to distribute the GAN training tasks for hyper-parameter sets on Cori using IPyParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenient fudge for python path\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "\n",
    "# External imports\n",
    "import ipyparallel as ipp\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "#from atlasgan.dataset import RPVImages\n",
    "#from atlasgan.trainers import DCGANTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          13553035 interacti       sh sfarrell  R       0:52      2 nid000[11-12]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "squeue -u sfarrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker IDs: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Cluster ID taken from job ID above\n",
    "job_id = 13553035\n",
    "cluster_id = 'cori_{}'.format(job_id)\n",
    "\n",
    "# Use default profile\n",
    "c = ipp.Client(timeout=60, cluster_id=cluster_id)\n",
    "print('Worker IDs:', c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyperparameter search tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config\n",
    "n_train = 1024\n",
    "input_file = '/global/cscratch1/sd/sfarrell/atlas_gan/data_split/RPV10_1400_850_01_train.npz'\n",
    "output_dir_base = '/global/cscratch1/sd/sfarrell/atlas_gan/AtlasDCGAN_notebook'\n",
    "\n",
    "os.makedirs(output_dir_base, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "n_hpo_trials = 2\n",
    "noise_dim = np.random.choice([16, 32, 64, 128], size=n_hpo_trials)\n",
    "n_filters = np.random.choice([8, 16, 32, 64, 128], size=n_hpo_trials)\n",
    "lr = np.random.choice([1e-5, 1e-4, 2e-4, 1e-3, 5e-3], size=n_hpo_trials)\n",
    "flip_rate = np.random.uniform(0, 0.2, size=n_hpo_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(input_file, output_dir, n_train,\n",
    "                    noise_dim, n_filters, lr, flip_rate,\n",
    "                    batch_size, n_epochs):\n",
    "    # Convenient fudge for python path\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    import os\n",
    "    import logging\n",
    "    from torch.utils.data import DataLoader\n",
    "    from atlasgan.dataset import RPVImages\n",
    "    from atlasgan.trainers import DCGANTrainer\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "    logging.basicConfig(level=logging.INFO, format=log_format)\n",
    "    \n",
    "    # Set up the data loader\n",
    "    scale = 4e6\n",
    "    dataset = RPVImages(input_file, n_samples=n_train, scale=scale)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    #print('Loaded data with shape: %s' % str(dataset.data.size()))\n",
    "    # Instantiate the trainer\n",
    "    trainer = DCGANTrainer(noise_dim=int(noise_dim), n_filters=int(n_filters),\n",
    "                           lr=lr, flip_rate=flip_rate, \n",
    "                           threshold=500./scale, image_norm=scale,\n",
    "                           output_dir=output_dir, cuda=False)\n",
    "    # Run the training\n",
    "    trainer.train(data_loader, n_epochs=n_epochs, n_save=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter trial 0 noise 32 filters 128 flip 0.193 lr 0.0010\n",
      "Hyperparameter trial 1 noise 64 filters 32 flip 0.013 lr 0.0050\n"
     ]
    }
   ],
   "source": [
    "# Load-balanced view\n",
    "lv = c.load_balanced_view()\n",
    "\n",
    "# Loop over hyper-parameter sets\n",
    "results = []\n",
    "for ihp in range(n_hpo_trials):\n",
    "    print('Hyperparameter trial %i noise %i filters %i flip %.3f lr %.4f' %\n",
    "          (ihp, noise_dim[ihp], n_filters[ihp], flip_rate[ihp], lr[ihp]))\n",
    "    output_dir = os.path.join(output_dir_base, 'hp_%i' % ihp)\n",
    "    result = lv.apply(build_and_train,\n",
    "                      input_file=input_file, output_dir=output_dir, n_train=n_train,\n",
    "                      noise_dim=noise_dim[ihp], n_filters=n_filters[ihp], lr=lr[ihp],\n",
    "                      flip_rate=flip_rate[ihp], batch_size=batch_size, n_epochs=n_epochs)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 2 / 2\n"
     ]
    }
   ],
   "source": [
    "print('Tasks completed: %i / %i' % (np.sum([ar.ready() for ar in results]), len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<AsyncResult: build_and_train:finished>,\n",
       " <AsyncResult: build_and_train:finished>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with shape: torch.Size([1024, 1, 64, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ar = results[0]\n",
    "print(ar.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg_id': '165b86a4-bb886678a23930a529f8df20',\n",
       " 'submitted': datetime.datetime(2018, 7, 8, 15, 48, 31, 589789, tzinfo=tzutc()),\n",
       " 'started': datetime.datetime(2018, 7, 8, 15, 48, 31, 595909, tzinfo=tzutc()),\n",
       " 'completed': datetime.datetime(2018, 7, 8, 16, 10, 25, 869777, tzinfo=tzutc()),\n",
       " 'received': datetime.datetime(2018, 7, 8, 16, 10, 25, 873703, tzinfo=tzutc()),\n",
       " 'engine_uuid': '33154006-e458055573ef4fa6f24b1898',\n",
       " 'engine_id': 1,\n",
       " 'follow': [],\n",
       " 'after': [],\n",
       " 'status': 'ok',\n",
       " 'execute_input': None,\n",
       " 'execute_result': None,\n",
       " 'error': None,\n",
       " 'stdout': 'Loaded data with shape: torch.Size([1024, 1, 64, 64])\\n',\n",
       " 'stderr': '',\n",
       " 'outputs': [],\n",
       " 'data': {}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with shape: torch.Size([1024, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "ar.display_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook example isn't doing much at the moment. In the HP tasks we're writing checkpoints for the model at every epoch, so all the information is there for later analysis.\n",
    "\n",
    "Coming soon I'll add some validation directly to the task so we can get an answer right away about the quality of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3",
   "language": "python",
   "name": "pytorch3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
